"""
================================================================================
AI-DRIVEN CUSTOMER SEGMENTATION - COMPLETE COLAB NOTEBOOK
SQL + K-Means Clustering + Streamlit Dashboard with ngrok
================================================================================
Run each cell in sequence in Google Colab
"""

# ============================================================================
# CELL 1: Install Dependencies
# ============================================================================
print("Installing required packages...")
!pip install streamlit pyngrok scikit-learn pandas sqlalchemy matplotlib seaborn plotly -q
!pip install --upgrade pyngrok -q
print("âœ“ All packages installed successfully!")

# ============================================================================
# CELL 2: Setup ngrok Authentication (with Input)
# ============================================================================
from getpass import getpass

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  GET YOUR NGROK TOKEN:                                                   â•‘
â•‘  1. Visit: https://dashboard.ngrok.com/get-started/your-authtoken       â•‘
â•‘  2. Sign up/login (free)                                                â•‘
â•‘  3. Copy your authtoken                                                 â•‘
â•‘  4. Paste it when prompted below                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Ask for ngrok token securely
NGROK_TOKEN = getpass("Enter your ngrok authtoken: ")

if NGROK_TOKEN and NGROK_TOKEN.strip():
    !ngrok authtoken {NGROK_TOKEN}
    print("âœ“ Ngrok authenticated successfully!")
else:
    print("âš ï¸  WARNING: No token provided. Dashboard launch will fail.")

# ============================================================================
# CELL 3: Import Libraries
# ============================================================================
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import warnings
warnings.filterwarnings('ignore')

print("âœ“ All libraries imported successfully!")

# ============================================================================
# CELL 4: Create Sample Database with SQL
# ============================================================================
def create_sample_database():
    """Generate realistic customer database with 4 tables"""
    
    conn = sqlite3.connect('customer_data.db')
    cursor = conn.cursor()
    
    # Drop existing tables
    cursor.execute('DROP TABLE IF EXISTS interactions')
    cursor.execute('DROP TABLE IF EXISTS transactions')
    cursor.execute('DROP TABLE IF EXISTS products')
    cursor.execute('DROP TABLE IF EXISTS customers')
    
    # Create tables
    cursor.execute('''
        CREATE TABLE customers (
            customer_id INTEGER PRIMARY KEY,
            name TEXT,
            email TEXT,
            signup_date DATE,
            country TEXT
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE products (
            product_id INTEGER PRIMARY KEY,
            product_name TEXT,
            category TEXT,
            price REAL
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE transactions (
            transaction_id INTEGER PRIMARY KEY,
            customer_id INTEGER,
            product_id INTEGER,
            transaction_date DATE,
            quantity INTEGER,
            total_amount REAL,
            FOREIGN KEY (customer_id) REFERENCES customers(customer_id),
            FOREIGN KEY (product_id) REFERENCES products(product_id)
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE interactions (
            interaction_id INTEGER PRIMARY KEY,
            customer_id INTEGER,
            interaction_type TEXT,
            interaction_date DATE,
            duration_seconds INTEGER,
            FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
        )
    ''')
    
    # Generate sample data
    np.random.seed(42)
    
    # Customers (1000 customers)
    print("Generating customers...")
    customers = []
    for i in range(1, 1001):
        signup_date = datetime.now() - timedelta(days=np.random.randint(1, 730))
        customers.append((i, f'Customer_{i}', f'customer{i}@email.com', 
                         signup_date.strftime('%Y-%m-%d'), 
                         np.random.choice(['USA', 'UK', 'Canada', 'Australia'])))
    
    cursor.executemany('INSERT INTO customers VALUES (?,?,?,?,?)', customers)
    
    # Products (50 products)
    print("Generating products...")
    categories = ['Electronics', 'Clothing', 'Home', 'Sports', 'Books']
    products = []
    for i in range(1, 51):
        products.append((i, f'Product_{i}', 
                        np.random.choice(categories),
                        round(np.random.uniform(10, 500), 2)))
    
    cursor.executemany('INSERT INTO products VALUES (?,?,?,?)', products)
    
    # Transactions (varied patterns for segmentation)
    print("Generating transactions...")
    transactions = []
    trans_id = 1
    for customer_id in range(1, 1001):
        # Create different customer behaviors
        if customer_id <= 200:  # High-value customers
            num_trans = np.random.randint(15, 30)
        elif customer_id <= 500:  # Medium customers
            num_trans = np.random.randint(5, 15)
        else:  # Low-value customers
            num_trans = np.random.randint(1, 5)
        
        for _ in range(num_trans):
            trans_date = datetime.now() - timedelta(days=np.random.randint(1, 365))
            product_id = np.random.randint(1, 51)
            quantity = np.random.randint(1, 5)
            amount = round(np.random.uniform(20, 300) * quantity, 2)
            transactions.append((trans_id, customer_id, product_id, 
                               trans_date.strftime('%Y-%m-%d'), quantity, amount))
            trans_id += 1
    
    cursor.executemany('INSERT INTO transactions VALUES (?,?,?,?,?,?)', transactions)
    
    # Interactions
    print("Generating interactions...")
    interactions = []
    inter_id = 1
    interaction_types = ['email_open', 'website_visit', 'support_ticket', 'survey_response']
    
    for customer_id in range(1, 1001):
        num_interactions = np.random.randint(0, 20)
        for _ in range(num_interactions):
            inter_date = datetime.now() - timedelta(days=np.random.randint(1, 365))
            interactions.append((inter_id, customer_id, 
                               np.random.choice(interaction_types),
                               inter_date.strftime('%Y-%m-%d'),
                               np.random.randint(10, 600)))
            inter_id += 1
    
    cursor.executemany('INSERT INTO interactions VALUES (?,?,?,?,?)', interactions)
    
    conn.commit()
    conn.close()
    
    # Display table stats
    conn = sqlite3.connect('customer_data.db')
    print("\n" + "="*70)
    print("DATABASE CREATED SUCCESSFULLY!")
    print("="*70)
    print(f"Customers:     {pd.read_sql_query('SELECT COUNT(*) as count FROM customers', conn)['count'][0]:,}")
    print(f"Products:      {pd.read_sql_query('SELECT COUNT(*) as count FROM products', conn)['count'][0]:,}")
    print(f"Transactions:  {pd.read_sql_query('SELECT COUNT(*) as count FROM transactions', conn)['count'][0]:,}")
    print(f"Interactions:  {pd.read_sql_query('SELECT COUNT(*) as count FROM interactions', conn)['count'][0]:,}")
    print("="*70)
    conn.close()

# Execute database creation
create_sample_database()

# ============================================================================
# CELL 5: SQL Data Preparation - RFM Analysis
# ============================================================================
def prepare_customer_features():
    """
    Advanced SQL queries to extract RFM (Recency, Frequency, Monetary) 
    and engagement features for machine learning
    """
    
    conn = sqlite3.connect('customer_data.db')
    
    # Complex SQL query with CTEs (Common Table Expressions)
    query = """
    WITH customer_rfm AS (
        SELECT 
            c.customer_id,
            c.name,
            -- RECENCY: Days since last transaction
            JULIANDAY('now') - JULIANDAY(MAX(t.transaction_date)) as recency_days,
            
            -- FREQUENCY: Number of transactions
            COUNT(DISTINCT t.transaction_id) as frequency,
            
            -- MONETARY: Total spending
            SUM(t.total_amount) as total_spend,
            AVG(t.total_amount) as avg_transaction_value,
            
            -- Additional features
            COUNT(DISTINCT t.product_id) as unique_products,
            MAX(t.total_amount) as max_purchase,
            MIN(t.total_amount) as min_purchase
            
        FROM customers c
        LEFT JOIN transactions t ON c.customer_id = t.customer_id
        GROUP BY c.customer_id, c.name
    ),
    
    customer_interactions AS (
        SELECT 
            customer_id,
            COUNT(*) as interaction_count,
            AVG(duration_seconds) as avg_interaction_duration,
            COUNT(DISTINCT interaction_type) as interaction_types,
            SUM(CASE WHEN interaction_type = 'email_open' THEN 1 ELSE 0 END) as email_opens,
            SUM(CASE WHEN interaction_type = 'website_visit' THEN 1 ELSE 0 END) as website_visits
        FROM interactions
        GROUP BY customer_id
    ),
    
    customer_tenure AS (
        SELECT 
            customer_id,
            JULIANDAY('now') - JULIANDAY(signup_date) as tenure_days
        FROM customers
    )
    
    SELECT 
        rfm.customer_id,
        rfm.name,
        COALESCE(rfm.recency_days, 365) as recency_days,
        COALESCE(rfm.frequency, 0) as frequency,
        COALESCE(rfm.total_spend, 0) as total_spend,
        COALESCE(rfm.avg_transaction_value, 0) as avg_transaction_value,
        COALESCE(rfm.unique_products, 0) as unique_products,
        COALESCE(rfm.max_purchase, 0) as max_purchase,
        COALESCE(rfm.min_purchase, 0) as min_purchase,
        COALESCE(i.interaction_count, 0) as interaction_count,
        COALESCE(i.avg_interaction_duration, 0) as avg_interaction_duration,
        COALESCE(i.interaction_types, 0) as interaction_types,
        COALESCE(i.email_opens, 0) as email_opens,
        COALESCE(i.website_visits, 0) as website_visits,
        t.tenure_days
    FROM customer_rfm rfm
    LEFT JOIN customer_interactions i ON rfm.customer_id = i.customer_id
    LEFT JOIN customer_tenure t ON rfm.customer_id = t.customer_id
    ORDER BY rfm.customer_id
    """
    
    df = pd.read_sql_query(query, conn)
    conn.close()
    
    print("\n" + "="*70)
    print("SQL DATA PREPARATION COMPLETED")
    print("="*70)
    print(f"Total customers: {len(df):,}")
    print(f"\nFeatures extracted: {len(df.columns)}")
    print(f"Feature columns:\n{list(df.columns)}")
    print(f"\nData shape: {df.shape}")
    print("\nSample statistics:")
    print(df[['recency_days', 'frequency', 'total_spend', 'interaction_count']].describe())
    print("="*70)
    
    return df

# Execute feature preparation
df_features = prepare_customer_features()

# Display sample data
print("\nSample of customer features:")
display(df_features.head(10))

# ============================================================================
# CELL 6: Feature Normalization & K-Means Clustering
# ============================================================================
def perform_clustering(df, n_clusters=5):
    """
    Normalize features and apply K-means clustering algorithm
    """
    
    # Select features for clustering (exclude IDs and names)
    feature_cols = ['recency_days', 'frequency', 'total_spend', 
                   'avg_transaction_value', 'unique_products', 'max_purchase',
                   'interaction_count', 'avg_interaction_duration', 
                   'interaction_types', 'tenure_days']
    
    X = df[feature_cols].copy()
    X = X.fillna(0)
    
    # Normalize features using StandardScaler
    print("\nNormalizing features...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Find optimal number of clusters
    print("Finding optimal number of clusters...")
    inertias = []
    silhouette_scores = []
    K_range = range(2, 11)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))
    
    # Plot elbow curve and silhouette scores
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    ax1.plot(K_range, inertias, 'bo-')
    ax1.set_xlabel('Number of Clusters (k)')
    ax1.set_ylabel('Inertia')
    ax1.set_title('Elbow Method')
    ax1.grid(True)
    
    ax2.plot(K_range, silhouette_scores, 'ro-')
    ax2.set_xlabel('Number of Clusters (k)')
    ax2.set_ylabel('Silhouette Score')
    ax2.set_title('Silhouette Score Method')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # Apply K-means with specified number of clusters
    print(f"\nApplying K-means with {n_clusters} clusters...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df['cluster'] = kmeans.fit_predict(X_scaled)
    
    silhouette_avg = silhouette_score(X_scaled, df['cluster'])
    
    print("\n" + "="*70)
    print("K-MEANS CLUSTERING COMPLETED")
    print("="*70)
    print(f"Number of clusters: {n_clusters}")
    print(f"Silhouette Score: {silhouette_avg:.3f}")
    print(f"\nCluster distribution:")
    print(df['cluster'].value_counts().sort_index())
    print("="*70)
    
    return df, kmeans, scaler, X_scaled, feature_cols

# Execute clustering
df_clustered, kmeans_model, scaler, X_scaled, feature_cols = perform_clustering(df_features, n_clusters=5)

# ============================================================================
# CELL 7: Cluster Analysis & Labeling
# ============================================================================
def analyze_clusters(df):
    """
    Analyze cluster characteristics and assign business labels
    """
    
    cluster_summary = df.groupby('cluster').agg({
        'recency_days': 'mean',
        'frequency': 'mean',
        'total_spend': 'mean',
        'avg_transaction_value': 'mean',
        'interaction_count': 'mean',
        'tenure_days': 'mean',
        'customer_id': 'count'
    }).round(2)
    
    cluster_summary.columns = ['Avg_Recency', 'Avg_Frequency', 'Avg_Spend', 
                               'Avg_Transaction', 'Avg_Interactions', 
                               'Avg_Tenure', 'Customer_Count']
    
    # Intelligent cluster labeling based on RFM characteristics
    cluster_labels = {}
    for cluster in cluster_summary.index:
        spend = cluster_summary.loc[cluster, 'Avg_Spend']
        freq = cluster_summary.loc[cluster, 'Avg_Frequency']
        recency = cluster_summary.loc[cluster, 'Avg_Recency']
        
        spend_median = cluster_summary['Avg_Spend'].median()
        freq_median = cluster_summary['Avg_Frequency'].median()
        recency_q75 = cluster_summary['Avg_Recency'].quantile(0.75)
        
        if spend > spend_median and freq > freq_median and recency < recency_q75:
            cluster_labels[cluster] = 'ğŸ’ Champions'
        elif spend > spend_median and recency > recency_q75:
            cluster_labels[cluster] = 'âš ï¸ At Risk'
        elif spend > spend_median:
            cluster_labels[cluster] = 'ğŸ’° Big Spenders'
        elif freq > freq_median and recency < recency_q75:
            cluster_labels[cluster] = 'ğŸŒŸ Loyal Customers'
        else:
            cluster_labels[cluster] = 'ğŸŒ± Potential Loyalists'
    
    df['segment_name'] = df['cluster'].map(cluster_labels)
    cluster_summary['Segment_Name'] = cluster_summary.index.map(cluster_labels)
    
    print("\n" + "="*70)
    print("CLUSTER ANALYSIS RESULTS")
    print("="*70)
    print(cluster_summary)
    print("="*70)
    
    # Visualize cluster characteristics
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Plot 1: Recency by segment
    df.boxplot(column='recency_days', by='segment_name', ax=axes[0,0])
    axes[0,0].set_title('Recency by Segment')
    axes[0,0].set_xlabel('Segment')
    axes[0,0].set_ylabel('Days Since Last Purchase')
    
    # Plot 2: Frequency by segment
    df.boxplot(column='frequency', by='segment_name', ax=axes[0,1])
    axes[0,1].set_title('Frequency by Segment')
    axes[0,1].set_xlabel('Segment')
    axes[0,1].set_ylabel('Number of Purchases')
    
    # Plot 3: Monetary by segment
    df.boxplot(column='total_spend', by='segment_name', ax=axes[1,0])
    axes[1,0].set_title('Monetary Value by Segment')
    axes[1,0].set_xlabel('Segment')
    axes[1,0].set_ylabel('Total Spend ($)')
    
    # Plot 4: Customer count by segment
    segment_counts = df['segment_name'].value_counts()
    axes[1,1].bar(range(len(segment_counts)), segment_counts.values)
    axes[1,1].set_xticks(range(len(segment_counts)))
    axes[1,1].set_xticklabels(segment_counts.index, rotation=45, ha='right')
    axes[1,1].set_title('Customer Count by Segment')
    axes[1,1].set_ylabel('Number of Customers')
    
    plt.suptitle('')
    plt.tight_layout()
    plt.show()
    
    return df, cluster_summary

# Execute cluster analysis
df_final, cluster_summary = analyze_clusters(df_clustered)

# ============================================================================
# CELL 8: 3D Visualization with PCA
# ============================================================================
# PCA for visualization
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

df_viz = df_final.copy()
df_viz['PC1'] = X_pca[:, 0]
df_viz['PC2'] = X_pca[:, 1]
df_viz['PC3'] = X_pca[:, 2]

# 3D scatter plot
fig = px.scatter_3d(df_viz, 
                    x='PC1', y='PC2', z='PC3',
                    color='segment_name',
                    size='total_spend',
                    hover_data=['customer_id', 'name', 'frequency', 'recency_days'],
                    title='Customer Segments in 3D Space (PCA)',
                    labels={'PC1': 'Principal Component 1', 
                           'PC2': 'Principal Component 2',
                           'PC3': 'Principal Component 3'})
fig.update_layout(height=700)
fig.show()

# RFM 3D scatter plot
fig2 = px.scatter_3d(df_final, 
                     x='recency_days', 
                     y='frequency', 
                     z='total_spend',
                     color='segment_name',
                     size='avg_transaction_value',
                     hover_data=['customer_id', 'name'],
                     title='Customer Segments in RFM Space',
                     labels={'recency_days': 'Recency (days)', 
                            'frequency': 'Frequency (purchases)', 
                            'total_spend': 'Monetary (total spend)'})
fig2.update_layout(height=700)
fig2.show()

print(f"\nâœ“ PCA explained variance ratio: {pca.explained_variance_ratio_}")
print(f"âœ“ Total variance explained: {sum(pca.explained_variance_ratio_):.2%}")

# ============================================================================
# CELL 9: Save Results & Create Streamlit App
# ============================================================================
# Save results
df_final.to_csv('customer_segments.csv', index=False)
cluster_summary.to_csv('cluster_summary.csv')

print("\nâœ“ Results saved:")
print("  - customer_segments.csv")
print("  - cluster_summary.csv")

# Create Streamlit app file
streamlit_code = '''import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

st.set_page_config(page_title="Customer Segmentation Dashboard", layout="wide")

@st.cache_data
def load_data():
    return pd.read_csv('customer_segments.csv')

@st.cache_data
def load_summary():
    return pd.read_csv('cluster_summary.csv')

df = load_data()
summary = load_summary()

st.title("ğŸ¯ AI-Driven Customer Segmentation Dashboard")
st.markdown("### Powered by K-Means Clustering & SQL Analytics")
st.markdown("---")

# Sidebar
st.sidebar.header("ğŸ” Filters")
selected = st.sidebar.multiselect("Select Segments", df['segment_name'].unique(), df['segment_name'].unique())
filtered = df[df['segment_name'].isin(selected)]

# Metrics
col1, col2, col3, col4 = st.columns(4)
col1.metric("Total Customers", f"{len(filtered):,}")
col2.metric("Avg Customer Value", f"${filtered['total_spend'].mean():,.2f}")
col3.metric("Avg Frequency", f"{filtered['frequency'].mean():.1f}")
col4.metric("Active Segments", len(selected))

st.markdown("---")

# Distribution
col1, col2 = st.columns(2)
with col1:
    st.subheader("ğŸ“Š Segment Distribution")
    counts = filtered['segment_name'].value_counts()
    fig = px.pie(values=counts.values, names=counts.index, hole=0.4)
    st.plotly_chart(fig, use_container_width=True)

with col2:
    st.subheader("ğŸ’° Revenue by Segment")
    revenue = filtered.groupby('segment_name')['total_spend'].sum().sort_values(ascending=False)
    fig = px.bar(x=revenue.index, y=revenue.values, color=revenue.values, color_continuous_scale='Viridis')
    fig.update_layout(showlegend=False, xaxis_title="", yaxis_title="Total Revenue ($)")
    st.plotly_chart(fig, use_container_width=True)

# RFM Analysis
st.subheader("ğŸ“ˆ RFM Analysis by Segment")
col1, col2, col3 = st.columns(3)

with col1:
    fig = px.box(filtered, x='segment_name', y='recency_days', color='segment_name')
    fig.update_layout(showlegend=False, xaxis_title="", yaxis_title="Days Since Last Purchase")
    st.plotly_chart(fig, use_container_width=True)

with col2:
    fig = px.box(filtered, x='segment_name', y='frequency', color='segment_name')
    fig.update_layout(showlegend=False, xaxis_title="", yaxis_title="Purchase Frequency")
    st.plotly_chart(fig, use_container_width=True)

with col3:
    fig = px.box(filtered, x='segment_name', y='total_spend', color='segment_name')
    fig.update_layout(showlegend=False, xaxis_title="", yaxis_title="Total Spend ($)")
    st.plotly_chart(fig, use_container_width=True)

# 3D Visualization
st.subheader("ğŸ¨ 3D Customer Visualization")
fig = px.scatter_3d(filtered, x='recency_days', y='frequency', z='total_spend',
                    color='segment_name', size='avg_transaction_value',
                    hover_data=['customer_id', 'name'])
fig.update_layout(height=600)
st.plotly_chart(fig, use_container_width=True)

# Summary Table
st.subheader("ğŸ“‹ Segment Summary Statistics")
st.dataframe(summary, use_container_width=True)

# Customer Details
st.subheader("ğŸ‘¥ Customer Details")
cols = ['customer_id', 'name', 'segment_name', 'recency_days', 'frequency', 
        'total_spend', 'avg_transaction_value', 'interaction_count']
st.dataframe(filtered[cols].sort_values('total_spend', ascending=False), use_container_width=True)

# Download
csv = filtered.to_csv(index=False)
st.download_button("ğŸ“¥ Download Data", csv, "customer_segments.csv", "text/csv")
'''

with open('streamlit_app.py', 'w') as f:
    f.write(streamlit_code)

print("âœ“ Streamlit app created: streamlit_app.py")

# ============================================================================
# CELL 10: Launch Streamlit Dashboard with ngrok
# ============================================================================
print("\n" + "="*70)
print("LAUNCHING STREAMLIT DASHBOARD")
print("="*70)

import subprocess
import time
from pyngrok import ngrok

# Kill any existing Streamlit processes
!pkill -f streamlit

# Start Streamlit in background
subprocess.Popen(['streamlit', 'run', 'streamlit_app.py', '--server.port', '8501'])

print("â³ Starting Streamlit server...")
time.sleep(5)

# Create ngrok tunnel
public_url = ngrok.connect(8501)

print("\n" + "="*70)
print("ğŸ‰ DASHBOARD IS READY!")
print("="*70)
print(f"ğŸŒ Access your dashboard at:\n")
print(f"   {public_url}")
print("\n" + "="*70)
print("\nğŸ’¡ Tips:")
print("  - The dashboard is fully interactive")
print("  - Use filters in the sidebar to explore segments")
print("  - Download filtered data as CSV")
print("  - Dashboard will stay active as long as this notebook is running")
print("="*70)

# ============================================================================
# CELL 11: Display Summary Statistics
# ============================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)
print(f"\nğŸ“Š Dataset Overview:")
print(f"   Total Customers: {len(df_final):,}")
print(f"   Number of Segments: {df_final['segment_name'].nunique()}")
print(f"   Total Revenue: ${df_final['total_spend'].sum():,.2f}")
print(f"   Average Customer Value: ${df_final['total_spend'].mean():,.2f}")

print(f"\nğŸ¯ Segment Breakdown:")
for segment in df_final['segment_name'].unique():
    seg_data = df_final[df_final['segment_name'] == segment]
    print(f"\n   {segment}:")
    print(f"      Customers: {len(seg_data):,} ({len(seg_data)/len(df_final)*100:.1f}%)")
    print(f"      Avg Spend: ${seg_data['total_spend'].mean():,.2f}")
    print(f"      Avg Frequency: {seg_data['frequency'].mean():.1f}")
    print(f"      Avg Recency: {seg_data['recency_days'].mean():.1f} days")

print(f"\nğŸ”¬ Model Performance:")
print(f"   Algorithm: K-Means Clustering")
print(f"   Features Used: {len(feature_cols)}")
print(f"   Silhouette Score: {silhouette_score(X_scaled, df_final['cluster']):.3f}")

print("\n" + "="*70)
print("âœ… PIPELINE COMPLETED SUCCESSFULLY!")
print("="*70)
print("\nğŸš€ Next Steps:")
print("   1. Explore the interactive dashboard at the URL above")
print("   2. Export customer segments for marketing campaigns")
print("   3. Monitor segment migration over time")
print("   4. Customize targeting strategies per segment")
print("="*70)
